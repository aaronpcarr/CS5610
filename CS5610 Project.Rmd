---
title: "CS 5610 Project"
author: "Aaron Carr & Daniel Reardon"
date: "4/8/2022"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Cardiovascular disease is the number 1 leading cause of death in the world, accounting for 31% of all deaths (17.9 million people every year.) Four fifths of these deaths are due to sudden episodes of heart attack and stroke, and one third of these occur in patients under 70 years old. As such, constructing a model which can predict these events prior to their occurrence has wide ranging potential benefits in the medical field. 

```{r}
library(boot)
library(MASS)
library(e1071)
library(caTools)
library(dplyr)
library(tree)
library(gbm)
library(ggplot2)
library(gridExtra)
library(randomForest)
```

The dataset which we use was constructed using 5 different datasets from Cleveland, Hungary, Switzerland, Long Beach California, and the Stalog Heart Data from the UCI Machine Learning Repository. It contains 918 different observations of which 11 common features are abstracted. These features are Age, Sex, Type of Chest Pain (Typical Angina, Atypical Angina, Non-Anginal Pain, and Asymptomatic), Resting Blood Pressure, Cholesterol, Fasting Blood Sugar (a binary measurement (1 if > 120 ml.dl, 0 otherwise), Resting Electrocardiogram Results (Normal, Having St-T wave abnormality, or showing at least probable left ventricular hypertrophy), Maximum Heart Rate achieved, Exercise Induced angina (yes or no), Oldpeak (as a numeric value), and ST-Slope (Up, down, or flat), and finally, the absence or presence of Heart Disease. The goal of our models is to predict the value of the Heart Disease variable using the values of the other variables as consistently and accurately as possible. 

```{r}
#Load Dataset
heart <- read.csv("heart.csv")
heart_copy = read.csv("heart.csv")
#View(heart)
heart <- mutate(heart, typical_angina = as.integer(ChestPainType == "TA")) %>%
  mutate(atypical_angina = as.integer(ChestPainType == "ATA")) %>%
  mutate(non_angina_pain = as.integer(ChestPainType == "NAP")) %>%
  mutate(st_abnorm =  as.integer(RestingECG == "ST")) %>%
  mutate(left_vent_hypertroph = as.integer(RestingECG == "LVH")) %>%
  mutate(ExerciseAngina = as.integer(ExerciseAngina == "Y")) %>%
  mutate(stslope_up = as.integer(ST_Slope == "Up")) %>%
  mutate(stslope_down = as.integer(ST_Slope == "Down")) %>%
  mutate(male = as.integer(Sex == "M")) %>%
  select(- c(ChestPainType, RestingECG, ST_Slope, Sex))

summary(heart)

#There appear to be some gaps in the data, where Cholesterol and RestingBP are labeled as zero, which is obviously just missing information
#We decided to construct small linear models to predict their values and replace them 

lm.chol_fit <- lm(Cholesterol  ~ . - HeartDisease, data = dplyr::filter(heart, Cholesterol != 0))
lm.bp_fit <- lm(RestingBP ~ . - HeartDisease, data = dplyr::filter(heart, RestingBP != 0))

PredictedCholesterol <- predict(lm.chol_fit, dplyr::filter(heart, Cholesterol == 0))
PredictedBP <- predict(lm.bp_fit, dplyr::filter(heart, RestingBP == 0))

heart <- mutate(heart, Cholesterol = replace(Cholesterol, Cholesterol == 0, PredictedCholesterol))
heart <- mutate(heart, RestingBP = replace(RestingBP, RestingBP == 0, PredictedBP))

summary(heart)

#Give it a look over
#View(heart)
names(heart)
dim(heart)
summary(heart)

#Check to see if there's any missing values
any(is.na(heart))

set.seed(97)
spl = sample.split(heart$HeartDisease, SplitRatio = 0.75)

heartTrain = subset(heart, spl==TRUE)
heartTest = subset(heart, spl==FALSE)

dim(heartTrain)
dim(heartTest)
```
Our first task involves wrangling the dataset so that it optimal for use by the models. Glancing over the dataset, it is clear that the some of the variables are categorical and consist of three or more categories; in situations such as these, it is vital to split these categories into binary categories. For the sake of clarity, this is explicitly performed by the dplyr pipeline above and not left to the models themselves. The chest pain type, resting ECG type, exercised induced angina and patient sex have all been reformatted such that the baseline patient is a female with no resting chest pain, a standard ECG, and no exercised induced chest pain.  
Next, when looking over a summary of the original dataset, it becomes apparent that while there are technically no missing values, some Cholesterol values (and a single Resting Blood Pressure value) are listed as 0, which is clearly erroneous. In order to best address this without skewing the data or removing a large chunk of training data, toy linear regression models were constructed to predict the appropriate values for these variables. In situations where the data appeared to be erroneous, the erroneous value was replaced with the predicted value. 

```{r}
#Heart Disease by sex
one = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = Sex, y = HeartDisease, fill = Sex)) + labs(y = "Heart Disease")
zero = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = Sex, y = 1 - HeartDisease, fill = Sex)) + labs(y = "Non-Heart Disease")

grid.arrange(one,zero, ncol= 2)
```

This dataset seems to suggest that Heart Disease is more common among men than women.

```{r}
#Heart disease by chest pain type
two = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = ChestPainType, y = HeartDisease, fill = ChestPainType)) + labs(y = "Heart Disease")
three = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = ChestPainType, y = 1 - HeartDisease, fill = ChestPainType)) + labs(y = "Non-Heart Disease")

grid.arrange(two,three, ncol= 2)
```


Asymptomatic individuals seem to dominate the heart disease group. There is a condition known as silent ischemia which restricts blood flow to the heart while the person feels no pain.


```{r}
#Heart disease by age and cholesterol
ggplot(data = heart) +
  geom_point(mapping = aes(y = Cholesterol, x = Age, color = factor(HeartDisease, labels = c("No","Yes"))))   +
      scale_color_brewer(palette = "Set1") +
      labs(color = "Heart Disease")
```

It is difficult to establish a relationship of age, cholesterol, and heart disease based on this scatterplot. It does appear that there that heart disease risk increases with age.

```{r}
#Heart disease by Resting ECG
four = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = RestingECG, y = HeartDisease, fill = RestingECG)) + labs(y = "Heart Disease")
five = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = RestingECG, y = 1 - HeartDisease, fill = RestingECG)) + labs(y = "Non-Heart Disease")

grid.arrange(four,five, ncol= 2)
```

RestingECG seems to have no affect on heart disease risk.

```{r}
#Heart Disease by ST_slope
six = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = ST_Slope, y = HeartDisease, fill = ST_Slope)) + labs(y = "Heart Disease")
seven = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = ST_Slope, y = 1 - HeartDisease, fill = ST_Slope)) + labs(y = "Non-Heart Disease")

grid.arrange(six,seven, ncol= 2)
```

A flat ST_slope seems to be a risk factor for heart disease.

```{r}
eight = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = ExerciseAngina, y = HeartDisease, fill = ExerciseAngina)) + labs(y = "Heart Disease")
nine = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = ExerciseAngina, y = 1 - HeartDisease, fill = ExerciseAngina)) + labs(y = "Non-Heart Disease")

grid.arrange(eight,nine, ncol= 2)
```

The presence of exercise angina appears to be an indicator of heart disease.

```{r}
#Logistic Regression (87%)
glm.fits <- glm(HeartDisease ~ .,
  family = binomial, data = heartTrain
)
summary(glm.fits)

glm.probs <- predict(glm.fits, type = "response")
glm.pred <- rep(0, 689)
glm.pred[glm.probs > .5] = 1

table(glm.pred, heartTrain$HeartDisease)
mean(glm.pred == heartTrain$HeartDisease)

glm.probs <- predict(glm.fits, type = "response", newdata = heartTest)
glm.predTest <- rep(0, 229)
glm.predTest[glm.probs > .5] = 1

table(glm.predTest, heartTest$HeartDisease)
mean(glm.predTest == heartTest$HeartDisease)

##Feature Selection algorithm
i <- glm(HeartDisease ~ 1,
                 family = binomial, data = heartTrain)

glm.new <- step(i, direction='both', scope=formula(glm.fits), trace=0)

summary(glm.new)

glm.probs <- predict(glm.new, type = "response", newdata = heartTest)
glm.predTest <- rep(0, 229)
glm.predTest[glm.probs > .5] = 1

table(glm.predTest, heartTest$HeartDisease)
mean(glm.predTest == heartTest$HeartDisease)

```

The Logistic function models the probability of the outcome being a "success" (in this case success means that a patient has heart disease). The estimates return the log odds of a success given a value. For example, the estimate for the sex of the patient is 1.642. Meaning that a male has e^1.642 higher odds (or about 5 times higher odds) of having heart disease than a female.

For a continuous variable like cholesterol, the interpretation is similar except it relates to an increase of one unit. For example if a patient has one cholesterol unit higher than another patient, that patient with have an increase in odds of e^0.003597 (or about 1.0036 times higher odds).

The algorithm used to select the features used in the model was the stepwise regression algorithm. The algorithm begins with nothing in the model but the intercept, and adds the most significant variable. It continues this step until there are no more significant variables. However, at each step it also looks to see that no variables have been made insignificant by the addition of new variables. While this method is not foolproof, it does generally give a good idea of what features to include.

The model seems to agree that the variables we identified as significant in the visualization stage should be included in the model. It also agrees with not including RestingECG in the model as that seemed to have no effect when looking at the bar chart.

The feature selected model performs almost identically to the model which includes all features, each having an accuracy score of around 88%. (In fact, on the test data, the feature selection model performs slightly better). 


```{r}
#Linear Discriminant Analysis (87%)
lda.fit <- lda(HeartDisease ~ ., data = heartTrain)
lda.fit
plot(lda.fit)

lda.pred <- predict(lda.fit, newdata = heartTest)
lda.class <- lda.pred$class
table(lda.class, heartTest$HeartDisease)
mean(lda.class == heartTest$HeartDisease)
```
The linear discriminant analysis model also displays a relatively high level of accuracy, at 87%. 


```{r}
#Quadratic Discriminant Analysis (86%)

qda.fit <- qda(HeartDisease ~ ., data = heartTrain)
qda.fit

qda.class <- predict(qda.fit, newdata = heartTest)$class
table(qda.class, heartTest$HeartDisease)
mean(qda.class == heartTest$HeartDisease)
```

The quadratic discriminant model has an accuracy of only around 85%, close to the other models but slightly weaker. This might be an indication that the boundary between categories can be represented linearly. 

```{r}
#Naive Bayes (87%)
nb.fit <- naiveBayes(HeartDisease ~ ., data = heartTrain)
nb.fit

nb.class <- predict(nb.fit, newdata = heartTest)
table(nb.class, heartTest$HeartDisease)
mean(nb.class == heartTest$HeartDisease)

```
The Naive Bayes model performs about as well as the quadratic discriminant model, at around 84%. 


```{r}
#Classification Tree 85%
tree.heart <- tree(as.factor(HeartDisease) ~ ., data = heartTrain)
summary(tree.heart)

tree:::print.tree(tree.heart)

tree.pred <- predict(tree.heart, heartTest, type = "class")
table(tree.pred, heartTest$HeartDisease)
(78+110)/229
```
The single decision tree model was relatively inaccurate, at only 82% accuracy.

```{r}
#Random Forest 84%
rf.heart <- randomForest(as.factor(HeartDisease) ~ ., data = heartTrain, importance = TRUE)
rf.pred <- predict(rf.heart, heartTest, type = "class")
table(rf.pred, heartTest$HeartDisease)
(86 + 114)/229
importance(rf.heart)

varImpPlot(rf.heart)
```
But the random forest model pushed the accuracy back up to around 87%. The random forest model also seems to indicate that upward sloping ST region is overwhelmingly indicative of the absence of heart disease, in agreement with the logistic model.

In summary, we have decided that the logistic model is the best model to use to solve this problem. It performs at or above the level of the other methods we tried, but it has the added benefit of being the easiest model to understand for someone who is not statistically literate. 