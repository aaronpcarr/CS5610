---
title: "CS 5610 Project"
author: "Aaron Carr & Daniel Reardon"
date: "4/8/2022"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
library(boot)
library(MASS)
library(e1071)
library(caTools)
library(dplyr)
library(tree)
library(gbm)
library(ggplot2)
library(gridExtra)
#library(randomForest)
```



```{r}
#Load Dataset
heart <- read.csv("heart.csv")
heart_copy = read.csv("heart.csv")
#View(heart)
heart <- mutate(heart, typical_angina = as.integer(ChestPainType == "TA")) %>%
  mutate(atypical_angina = as.integer(ChestPainType == "ATA")) %>%
  mutate(non_angina_pain = as.integer(ChestPainType == "NAP")) %>%
  mutate(st_abnorm =  as.integer(RestingECG == "ST")) %>%
  mutate(left_vent_hypertroph = as.integer(RestingECG == "LVH")) %>%
  mutate(ExerciseAngina = as.integer(ExerciseAngina == "Y")) %>%
  mutate(stslope_up = as.integer(ST_Slope == "Up")) %>%
  mutate(stslope_down = as.integer(ST_Slope == "Down")) %>%
  mutate(male = as.integer(Sex == "M")) %>%
  select(- c(ChestPainType, RestingECG, ST_Slope, Sex))

summary(heart)

#There appear to be some gaps in the data, where Cholesterol and RestingBP are labeled as zero, which is obviously just missing information
#We decided to construct small linear models to predict their values and replace them 

lm.chol_fit <- lm(Cholesterol  ~ ., data = dplyr::filter(heart, Cholesterol != 0))
lm.bp_fit <- lm(RestingBP ~ ., data = dplyr::filter(heart, RestingBP != 0))

PredictedCholesterol <- predict(lm.chol_fit, dplyr::filter(heart, Cholesterol == 0))
PredictedBP <- predict(lm.bp_fit, dplyr::filter(heart, RestingBP == 0))

heart <- mutate(heart, Cholesterol = replace(Cholesterol, Cholesterol == 0, PredictedCholesterol))
heart <- mutate(heart, RestingBP = replace(RestingBP, RestingBP == 0, PredictedBP))

summary(heart)

#Give it a look over
#View(heart)
names(heart)
dim(heart)
summary(heart)

#Check to see if there's any missing values
any(is.na(heart))

set.seed(97)
spl = sample.split(heart$HeartDisease, SplitRatio = 0.75)

heartTrain = subset(heart, spl==TRUE)
heartTest = subset(heart, spl==FALSE)

dim(heartTrain)
dim(heartTest)
```

```{r}
#Heart Disease by sex
one = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = Sex, y = HeartDisease, fill = Sex)) + labs(y = "Heart Disease")
zero = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = Sex, y = 1 - HeartDisease, fill = Sex)) + labs(y = "Non-Heart Disease")

grid.arrange(one,zero, ncol= 2)
```

This dataset seems to suggest that Heart Disease is more common among men than women.

```{r}
#Heart disease by chest pain type
two = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = ChestPainType, y = HeartDisease, fill = ChestPainType)) + labs(y = "Heart Disease")
three = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = ChestPainType, y = 1 - HeartDisease, fill = ChestPainType)) + labs(y = "Non-Heart Disease")

grid.arrange(two,three, ncol= 2)
```


Asymptomatic individuals seem to dominate the heart disease group. There is a condition known as silent ischemia which restricts blood flow to the heart while the person feels no pain.


```{r}
#Heart disease by age and cholesterol
ggplot(data = heart) +
  geom_point(mapping = aes(y = Cholesterol, x = Age, color = factor(HeartDisease, labels = c("No","Yes"))))   +
      scale_color_brewer(palette = "Set1") +
      labs(color = "Heart Disease")
```

It is difficult to establish a relationship of age, cholesterol, and heart disease based on this scatterplot. It does appear that there that heart disease risk increases with age

```{r}
#Heart disease by Resting ECG
four = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = RestingECG, y = HeartDisease, fill = RestingECG)) + labs(y = "Heart Disease")
five = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = RestingECG, y = 1 - HeartDisease, fill = RestingECG)) + labs(y = "Non-Heart Disease")

grid.arrange(four,five, ncol= 2)
```

RestingECG seems to have no affect on heart disease risk.

```{r}
#Heart Disease by ST_slope
six = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = ST_Slope, y = HeartDisease, fill = ST_Slope)) + labs(y = "Heart Disease")
seven = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = ST_Slope, y = 1 - HeartDisease, fill = ST_Slope)) + labs(y = "Non-Heart Disease")

grid.arrange(six,seven, ncol= 2)
```

A flat ST_slope seems to be a risk factor for heart disease

```{r}
eight = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = ExerciseAngina, y = HeartDisease, fill = ExerciseAngina)) + labs(y = "Heart Disease")
nine = ggplot(data = heart_copy) +
 geom_col(mapping = aes(x = ExerciseAngina, y = 1 - HeartDisease, fill = ExerciseAngina)) + labs(y = "Non-Heart Disease")

grid.arrange(eight,nine, ncol= 2)
```

The presence of exercise angina appears to be an indicator of heart disease.

```{r}
#Logistic Regression (87%)
glm.fits <- glm(HeartDisease ~ .,
  family = binomial, data = heartTrain
)
cv.err <- cv.glm(heartTrain, glm.fits)
cv.err$delta
summary(glm.fits)

glm.probs <- predict(glm.fits, type = "response")
glm.pred <- rep(0, 689)
glm.pred[glm.probs > .5] = 1

table(glm.pred, heartTrain$HeartDisease)
mean(glm.pred == heartTrain$HeartDisease)

glm.probs <- predict(glm.fits, type = "response", newdata = heartTest)
glm.predTest <- rep(0, 229)
glm.predTest[glm.probs > .5] = 1

table(glm.predTest, heartTest$HeartDisease)
mean(glm.predTest == heartTest$HeartDisease)

##Feature Selection algorithm
i <- glm(HeartDisease ~ 1,
                 family = binomial, data = heartTrain)

glm.new <- step(i, direction='both', scope=formula(glm.fits), trace=0)

summary(glm.new)

glm.probs <- predict(glm.new, type = "response", newdata = heartTest)
glm.predTest <- rep(0, 229)
glm.predTest[glm.probs > .5] = 1

table(glm.predTest, heartTest$HeartDisease)
mean(glm.predTest == heartTest$HeartDisease)

```

The Logistic function models the probability of the outcome being a "success" (in this case success means that a patient has heart disease). The estimates return the log odds of a success given a value. For example, the estimate for the sex of the patient is 1.642. Meaning that a male has e^1.642 higher odds (or about 5 times higher odds) of having heart disease than a female.

For a continuous variable like cholesterol, the interpretation is similar except it relates to an increase of one unit. For example if a patient has one cholesterol unit higher than another patient, that patient with have an increase in odds of e^0.003597 (or about 1.0036 times higher odds).

The algorithm used to select the features used in the model was the stepwise regression algorithm. The algorithm begins with nothing in the model but the intercept, and adds the most significant variable. It continues this step until there are no more significant variables. However, at each step it also looks to see that no variables have been made insignificant by the addition of new variables. While this method is not foolproof, it does generally give a good idea of what features to include.

The model seems to agree that the variables we identified as significant in the visualization stage should be included in the model. It also agrees with not including RestingECG in the model as that seemed to have no effect when looking at the bar chart.


```{r}
#Linear Discriminant Analysis (87%)
lda.fit <- lda(HeartDisease ~ ., data = heartTrain)
lda.fit
plot(lda.fit)

lda.pred <- predict(lda.fit, newdata = heartTest)
lda.class <- lda.pred$class
table(lda.class, heartTest$HeartDisease)
mean(lda.class == heartTest$HeartDisease)
```



```{r}
#Quadratic Discriminant Analysis (86%)

qda.fit <- qda(HeartDisease ~ ., data = heartTrain)
qda.fit

qda.class <- predict(qda.fit, newdata = heartTest)$class
table(qda.class, heartTest$HeartDisease)
mean(qda.class == heartTest$HeartDisease)
```



```{r}
#Naive Bayes (87%)
#nb.fit <- naiveBayes(HeartDisease ~ ., data = heartTrain)
#nb.fit

#nb.class <- predict(nb.fit, newdata = heartTest)
#table(nb.class, heartTest$HeartDisease)
#mean(nb.class == heartTest$HeartDisease)

```



```{r}
#Classification Tree 85%
tree.heart <- tree(as.factor(HeartDisease) ~ ., data = heartTrain)
summary(tree.heart)

tree:::print.tree(tree.heart)

tree.pred <- predict(tree.heart, heartTest, type = "class")
table(tree.pred, heartTest$HeartDisease)
(72+117)/229
```

```{r}
#Random Forest 84%
#rf.heart <- randomForest(as.factor(HeartDisease) ~ ., data = heartTrain, importance = TRUE)
#rf.pred <- predict(rf.heart, heartTest, type = "class")
#table(tree.pred, heartTest$HeartDisease)
#(72 + 117)/229
#importance(rf.heart)

#varImpPlot(rf.heart)
```

```{r}
# boost.heart <- gbm(HeartDisease ~ ., data = heartTrain, distribution = "bernoulli")
# boost.pred <- predict(boost.heart, heartTest, type = "response")
# 

```